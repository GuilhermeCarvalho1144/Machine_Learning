{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tnwbph1V-k2h",
    "outputId": "925f0259-556b-41f2-d2f8-395b604c0a32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uJ_YKEMn-k2u"
   },
   "outputs": [],
   "source": [
    "################################### LINEAR REGRESSION ###########################################\n",
    "## GUILHERME CARVALHO PEREIRA\n",
    "## IMPORTING LIBARIES\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import linear_model\n",
    "import sklearn.preprocessing\n",
    "import math,time, quandl\n",
    "from datetime import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import pickle\n",
    "\n",
    "## DEFINING THE STYLE OF THE PLOTS\n",
    "style.use('ggplot')\n",
    "\n",
    "## GETING THE DATE SET FORM QUANDL\n",
    "quandl.ApiConfig.api_key = 'G9SiyZ49oi4T_KYraTQs'\n",
    "df = quandl.get('WIKI/GOOGL', index_col='Date', parse_dates=True)\n",
    "## PREVIEWING THE DATA\n",
    "#print(df_raw.head())\n",
    "\n",
    "## GETING THE FEATURES MORE MEANINGFUL\n",
    "df = df[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume']]\n",
    "\n",
    "#################################################################################################\n",
    "## DEFINING THE SPECIAL RELATIONSHIP BETWEEN THE FEATURES\n",
    "\n",
    "## HIGH_LOW_PERCENT (HL_PCT) => THIS FEATURE GIVE US THE PERCENT VOLATILITY OF THE STOCKS\n",
    "df['HL_PCT'] = (df['Adj. High'] - df['Adj. Close'])/df['Adj. Close'] *100 \n",
    "\n",
    "## PERCENT_CHANGE (PCT_CHANGE) => THIS FEATURE GIVE US THE DAILY MOVE OF THE STOCKS\n",
    "df['PCT_change'] = (df['Adj. Close'] - df['Adj. Open']) / df['Adj. Open']*100.0\n",
    "\n",
    "## DEFINING THE DATAFRAME WITH THE SPECIAL RELATIONSHIPS\n",
    "df = df[['Adj. Close','HL_PCT','PCT_change','Adj. Volume']]\n",
    "## PREVIEWING THE DATA\n",
    "#print(df.head())\n",
    "\n",
    "## DEFINING THE LABEL... THE 'THING' WE WANT TO PREDICT\n",
    "forecast_col = 'Adj. Close'\n",
    "\n",
    "## FILLING THE NaN DATA ON THE DATASET\n",
    "df.fillna(-99999, inplace=True)\n",
    "\n",
    "## GETING 10% OF THE DATAFRAME AND TRY TO PREDICT\n",
    "pct_data = 0.01  ##GIVE HOW MUCH OF THE DATASET WE ARE TRYING TO PREDICT\n",
    "forecast_out = int(math.ceil(pct_data*len(df)))\n",
    "\n",
    "## ADD THE LABEL TO THE DATAFRAME\n",
    "df['Label'] = df[forecast_col].shift(-forecast_out)  ##JUST THE COLLUMS 10 DAYS ON THE FUTURE\n",
    "## PREVIEWING THE DATA\n",
    "#print(df.head())\n",
    "\n",
    "## DEFINING X AND y FOR OUR HYPOTHESIS\n",
    "X = np.array(df.drop(['Label'], 1))\n",
    "\n",
    "## SCALING OUR FEATURES\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "\n",
    "## DEFINING THE DATA FRO THE FORECAST\n",
    "X_lately = X[-forecast_out:]  ##DATA TO FORECAST\n",
    "X = X[:-forecast_out]\n",
    "\n",
    "## DEVIDING OUR DATA INTO TRAINING AND DATA SET...TESTE SIZE IN %\n",
    "df.dropna(inplace = True)\n",
    "y = np.array(df['Label'])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "## DEFINING OUR LINEAR REGRESSION HYPOTHESIS\n",
    "hypo = linear_model.LinearRegression()\n",
    "hypo.fit(X_train, y_train)\n",
    "\n",
    "############################################################################################\n",
    "'''' SAVING THE HYPOTHESIS IN A PICKLE FILE TO AVOID TRAIN EVERYTIME THAT THE CODE RUN\n",
    "     IF IS THE SECOND TIME THAT YOU RUN THIS CODE IS A GOOD IDEA UNCOMMENT THIS PART AND\n",
    "     COMMENT THE TW LINES ABOVE\n",
    "'''\n",
    "#with open('linearregression.pickle', 'wb') as file_name:\n",
    "#    pickle.dump(hypo, file_name)\n",
    "#pickle_in = open('linearregression.pickle', 'rb')\n",
    "#hypo = pickle_in\n",
    "############################################################################################\n",
    "\n",
    "## COMPUTATING HOW ACCURATE THE FIT IS\n",
    "accuracy = hypo.score(X_test, y_test)\n",
    "##print'THIS ALGORITHM PREDICTS THE OUTPUT WITH A ACCURACY OF {}%'.format(accuracy*100)\n",
    "## PREDICTION THE NEXT PRICE OF THE STOCKS\n",
    "y_predict = hypo.predict(X_lately)\n",
    "\n",
    "#################################################################################################\n",
    "\n",
    "## PREPARING THE DATA SET TO BE PLOT\n",
    "df['Forecast'] = np.nan\n",
    "df.index\n",
    "last_date = df.iloc[-1].name\n",
    "last_unix = time.mktime(time.strptime(str(last_date), \"%Y-%m-%d %H:%M:%S\")) # .timestamp()\n",
    "one_day = 86400\n",
    "next_unix = last_unix + one_day\n",
    "\n",
    "## AJUSTING THE DATA IN THE X AXIS\n",
    "for i in y_predict:\n",
    "    next_date = dt.fromtimestamp(next_unix)\n",
    "    next_unix += one_day\n",
    "    df.loc[next_date] = [np.nan for _ in range(len(df.columns)-1)] + [i]\n",
    "\n",
    "##PLOTING THE DATA AND THE PREDICTION\n",
    "df['Adj. Close'].plot()\n",
    "df['Forecast'].plot()\n",
    "plt.legend(loc=4)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IDdO9cTd-k20"
   },
   "outputs": [],
   "source": [
    "########################## LINEAR REGRESSION FUNCTION FROM SCRATCH ##########################\n",
    "from statistics import mean\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "''' THIS FUNCTION CALCULATES THE BEST FIT LINE TO A REGRESSION MODEL\n",
    "    THE FUNCTION TAKES TWO PARAMETES:\n",
    "    X -> DATA TO BE FIT AS THE x AXIS : DATA TYPE => numpy.array\n",
    "    y -> DATA TO BE FIT AS THE y AXIS : DATA TYPE => numpy.array\n",
    "'''\n",
    "def best_fit_line(X,y):\n",
    "    ## COMPUTATING m\n",
    "    m = (((mean(X)*mean(y)) - mean(X*y))/\n",
    "        ((mean(X)**2)-mean(X**2)))\n",
    "    b = mean(y) - m*mean(X)\n",
    "    ## RETURNING THE SLOPE AND INTERCEPT\n",
    "    return m, b\n",
    "\n",
    "\n",
    "''' THIS FUNCTION CALCULATES THE SQUARED ERROR TO A REGRESSION MODEL\n",
    "    THE FUNCTION TAKES TWO PARAMETES:\n",
    "    y -> DATA TO BE FIT AS THE y AXIS : DATA TYPE => numpy.array\n",
    "    regression_model -> DATA PREDICT USING THE best_fit_line function : DATA TYPE => numpy.array\n",
    "'''\n",
    "def squared_error(y, regression_modele):\n",
    "    return sum((regression_model - y)*(regression_model - y))\n",
    "\n",
    "''' THIS FUNCTION CALCULATES THE COEFICIENT OF DETERMINATION TO A REGRESSION MODEL\n",
    "    THIS COEFICIENT TELLS US HOW \"GOOD\" IS THE REGRESSION MODEL...GOOD MODEL OR NOT\n",
    "    A GOOD COEFICENT VALUE IS THE CLOSE TO 1 AS POSSIBLE\n",
    "    THE FUNCTION TAKES TWO PARAMETES:\n",
    "    y -> DATA TO BE FIT AS THE y AXIS : DATA TYPE => numpy.array\n",
    "    regression_model -> DATA PREDICT USING THE best_fit_line function : DATA TYPE => numpy.array\n",
    "'''\n",
    "def coef_of_determination(y, regression_model):\n",
    "    y_mean = [mean(y_line) for y_line in y]\n",
    "    \n",
    "    squared_error_regression = squared_error(y, regression_model)\n",
    "    squared_error_mean = squared_error(y, y_mean)\n",
    "    \n",
    "    print y_mean, squared_error_regression, squared_error_mean\n",
    "    \n",
    "    return (1-(squared_error_regression/squared_error_mean))\n",
    "\n",
    "## TESTING DATA\n",
    "\n",
    "X = np.array([1,2,3,4,5,6], dtype=np.float64)\n",
    "y = np.array([5,4,6,5,6,7], dtype=np.float64)\n",
    "\n",
    "## CALLING THE FUNCTION\n",
    "\n",
    "[m,b] = best_fit_line(X,y)\n",
    "\n",
    "print m, b\n",
    "\n",
    "## LINE FIT\n",
    "\n",
    "regression_model = [(m*x)+b for x in X]\n",
    "\n",
    "## TESTING HOW GOOD THE LINE FITS THIS DATA\n",
    "\n",
    "r_squared = coef_of_determination(y, regression_model)\n",
    "print r_squared\n",
    "\n",
    "## PLOTING THE DATA\n",
    "plt.scatter(X,y)\n",
    "plt.plot(X,regression_model)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HGGjqCYZ-k24"
   },
   "outputs": [],
   "source": [
    "##################################### CLASSIFICATION ###########################################\n",
    "## GUILHERME CARVALHO PEREIRA\n",
    "## IMPORTING LIBARIES\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import neighbors\n",
    "import sklearn.preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## DEFINING THE DATAFRAME\n",
    "df = pd.read_csv('breast-cancer-wisconsin.data')\n",
    "\n",
    "## PREVIEW \n",
    "#print (df.head())\n",
    "\n",
    "##CLEANING THE DATAFRAME\n",
    "df.replace('?', -99999, inplace=True) ##REPLACE THE MISSING DATA\n",
    "df.drop(['id'], 1, inplace=True) ##REMOVING THE ID COL\n",
    "\n",
    "## PREVIEW\n",
    "#print(df.head())\n",
    "\n",
    "## DEFINING THE FEATURES\n",
    "X = np.array(df.drop(['class'],1))\n",
    "\n",
    "## DEFINING THE LABEL\n",
    "y = np.array(df['class'])\n",
    "\n",
    "## DEFING THE TRAIN AND TEST SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "## CREATING THE K-NEAREST NEIIGHBORGS\n",
    "hypo = neighbors.KNeighborsClassifier()\n",
    "hypo.fit(X_train, y_train)\n",
    "\n",
    "## TESTING THE HYPOTHESIS ACCURACY\n",
    "accuracy = hypo.score(X_test, y_test)\n",
    "print 'THE ACCURACY OF THE K-NEARREST NEIGHBORGS IS {}%'.format(accuracy*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W0lJC00l-k29",
    "outputId": "6b0519e0-8046-40fe-8bbf-3dca60e8f29e",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ACCURACY OF THE K-NEARREST NEIGHBORGS IS 94.964028777%\n"
     ]
    }
   ],
   "source": [
    "########################## K-NEARREST NEIGHBORGS FROM SCRATCH ##############################\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import random\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "## DEFINING THE STYLE\n",
    "style.use('fivethirtyeight')\n",
    "\n",
    "'''\n",
    "## FIRST TEST\n",
    "dataset = {'k':[[1,2],[2,3],[3,1]], 'r':[[6,5],[7,7],[8,6]]}\n",
    "new_features = [5,7]\n",
    "''' \n",
    "\n",
    "''' PREVIEWING THE DATASET\n",
    "## PLOTING THE DATASET\n",
    "[[plt.scatter(ii[0], ii[1], s=100, color=i) for ii in dataset[i]] for i in dataset]\n",
    "'''\n",
    "## SAME FOR LOOP\n",
    "'''for i in dataset:\n",
    "       for ii in dataset[i]:\n",
    "           plt.scatter(ii[0], ii[1], s=100, color=i)\n",
    "'''\n",
    "'''\n",
    "## POLTING THE NEW FEATURE\n",
    "plt.scatter(new_features[0], new_features[1], s=100)\n",
    "'''\n",
    "\n",
    "\n",
    "def k_nearest_neighborgs(data, predict, k=3):\n",
    "    if len(data) >= k:\n",
    "        warnings.warn('**THE K IS SET TO A VALUE LESS THAN TOTAL VOTING GROUP!**')\n",
    "    distance = []\n",
    "    for group in data:\n",
    "        for features in data[group]:\n",
    "            euclid_dist = np.linalg.norm(np.array(features) - np.array(predict))\n",
    "            distance.append([euclid_dist, group])\n",
    "    \n",
    "    votes = [i[1] for i in sorted(distance)[:k]]\n",
    "    vote_result = Counter(votes).most_common(1)[0][0]\n",
    "    return vote_result\n",
    "\n",
    "############################################################################################\n",
    "## DEFINING THE DATAFRAME\n",
    "df = pd.read_csv('breast-cancer-wisconsin.data')\n",
    "\n",
    "## CLEANING THE DATAFRAME\n",
    "df.replace('?', -99999, inplace=True) ##REPLACE THE MISSING DATA\n",
    "df.drop(['id'], 1, inplace=True) ##REMOVING THE ID COL\n",
    "\n",
    "## SHUFFLING THE DATASET\n",
    "full_data = df.astype(float).values.tolist()\n",
    "random.shuffle(full_data)\n",
    "\n",
    "## DEFENING THE TRAIN AND TEST DATASETS\n",
    "test_size = 0.2  ##  20% OF THE FULL DATA\n",
    "train_set = {2:[],4:[]}\n",
    "test_set = {2:[],4:[]}\n",
    "train_data = full_data[:-int(test_size*len(full_data))]\n",
    "test_data = full_data[-int(test_size*len(full_data)):]\n",
    "for i in train_data:  ##  PASSING THE DATA TO THE DIC\n",
    "    train_set[i[-1]].append(i[:-1])\n",
    "for i in test_data:   ##  PASSING THE DATA TO THE DIC\n",
    "    test_set[i[-1]].append(i[:-1])\n",
    "\n",
    "    \n",
    "## CLASSICATIONG THE DATASET\n",
    "correct = 0\n",
    "total = 0\n",
    "for group in test_set:\n",
    "    for data in test_set[group]:\n",
    "        vote = k_nearest_neighborgs(train_set, data, k=5)\n",
    "        if group == vote:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "            \n",
    "## PRINTING THE ACCURACY OF THE CODE\n",
    "print 'THE ACCURACY OF THE K-NEARREST NEIGHBORGS IS {}%'.format((float(correct)/total)*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UapWdDSh-k3B",
    "outputId": "26ffce5c-c7e0-4dc8-b93e-ec4e23c8a370"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ACCURACY OF THE SVM IS 95.7142857143%\n"
     ]
    }
   ],
   "source": [
    "######################################### SVM ##############################################\n",
    "## GUILHERME CARVALHO PEREIRA\n",
    "## IMPORTING LIBARIES\n",
    "from sklearn.model_selection import cross_val_predict, train_test_split\n",
    "from sklearn import svm\n",
    "import sklearn.preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## DEFINING THE DATAFRAME\n",
    "df = pd.read_csv('breast-cancer-wisconsin.data')\n",
    "\n",
    "## PREVIEW \n",
    "#print (df.head())\n",
    "\n",
    "##CLEANING THE DATAFRAME\n",
    "df.replace('?', -99999, inplace=True) ##REPLACE THE MISSING DATA\n",
    "df.drop(['id'], 1, inplace=True) ##REMOVING THE ID COL\n",
    "\n",
    "## PREVIEW\n",
    "#print(df.head())\n",
    "\n",
    "## DEFINING THE FEATURES\n",
    "X = np.array(df.drop(['class'],1))\n",
    "\n",
    "## DEFINING THE LABEL\n",
    "y = np.array(df['class'])\n",
    "\n",
    "## DEFING THE TRAIN AND TEST SET\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "## CREATING THE K-NEAREST NEIIGHBORGS\n",
    "hypo = svm.SVC()\n",
    "hypo.fit(X_train, y_train)\n",
    "\n",
    "## TESTING THE HYPOTHESIS ACCURACY\n",
    "accuracy = hypo.score(X_test, y_test)\n",
    "print 'THE ACCURACY OF THE SVM IS {}%'.format(accuracy*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72upzWLe-k3G",
    "outputId": "1a086234-7b89-4fd4-e305-9790cdd8b5be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTIMIZED A STEP...\n",
      "OPTIMIZED A STEP...\n",
      "OPTIMIZED A STEP...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f952d3f6d30>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############################### SVM FROM SCRATCH ###########################################\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import pandas as pd\n",
    "style.use('ggplot')\n",
    "############################################################################################\n",
    "class Support_Vector_Machine:\n",
    "    '''\n",
    "    CONSTRUCTOR METHOD INICIALIZE THE OBJECT\n",
    "    ''' \n",
    "    def __init__(self, visualization=True):\n",
    "        self.visualization = visualization\n",
    "        self.colors = {-1:'b', 1:'r'}\n",
    "        \n",
    "        ## FIRST CHECK\n",
    "        if visualization:\n",
    "            self.figure = plt.figure()\n",
    "            self.ax = self.figure.add_subplot(1,1,1)  ##EQ TO SUBPLOT ON MATLAB\n",
    "    \n",
    "## METHODS    \n",
    "    '''\n",
    "    THIS METHOD COMPUTE THE HYPERPLANES GIVEN A DATASET. THIS METHOD TAKES DATASET IN DICTIONARY FORMAT\n",
    "    THIS METHOD CALCULATE THE VECTOR W AND THE BIAS b\n",
    "    THE HYPERPLANE EQUATIONS IS GIVEM BY W*X+b\n",
    "    '''\n",
    "    def train(self, data):\n",
    "        self.data = data\n",
    "        ## DICTIONARY WITH THE MAGNITUDES OF THE 'FIT' VECTORES\n",
    "        opt_dict = {}\n",
    "        \n",
    "        transforms = [[1,1],[-1,1],[-1,-1],[1,-1]]\n",
    "        \n",
    "        all_data = []\n",
    "        \n",
    "        for yi in self.data:\n",
    "            for featureset in self.data[yi]:\n",
    "                for features in featureset:\n",
    "                    all_data.append(features)\n",
    "        \n",
    "        self.max_features_val = max(all_data)\n",
    "        self.min_features_val = min(all_data)\n",
    "        \n",
    "        all_data = None\n",
    "        \n",
    "        step_sizes = [self.max_features_val*0.1, self.max_features_val*0.01, \n",
    "                      self.max_features_val*0.01]  ## ABOVE THIS POINT IS COMPUTATIONAL EXPENSIVE\n",
    "        \n",
    "        ## COMPUTATIONAL EXPENSIVE\n",
    "        b_range_multiple = 5\n",
    "        \n",
    "        ##\n",
    "        b_multiple = 5\n",
    "        \n",
    "        ##\n",
    "        latest_optimum = self.max_features_val*10\n",
    "        \n",
    "        ##\n",
    "        for step in step_sizes:\n",
    "            W = np.array([latest_optimum,latest_optimum])\n",
    "            optimized = False  ## CONVEX PROBLEM\n",
    "            while not optimized:\n",
    "                for b in np.arange(-1*(self.max_features_val*b_range_multiple), self.max_features_val*b_range_multiple, step*b_multiple):\n",
    "                    for transformation in transforms:\n",
    "                        w_t = W*transformation\n",
    "                        found_option = True\n",
    "                        ##\n",
    "                        ##\n",
    "                        ##\n",
    "                        for i in self.data:\n",
    "                            for xi in self.data[i]:\n",
    "                                yi = i\n",
    "                                if not yi*(np.dot(w_t,xi)+b) >= 1:\n",
    "                                    found_option = False\n",
    "                        ##\n",
    "                        ##\n",
    "                        if found_option:\n",
    "                            opt_dict[np.linalg.norm(w_t)] = [w_t, b]\n",
    "                \n",
    "                ##\n",
    "                ##\n",
    "                if W[0] < 0:\n",
    "                    optimized = True\n",
    "                    print('OPTIMIZED A STEP...')\n",
    "                else:\n",
    "                    W = W-step\n",
    "            ##\n",
    "            ##\n",
    "            norms = sorted(n for n in opt_dict)\n",
    "            \n",
    "            ##\n",
    "            ##\n",
    "            opt_choice = opt_dict[norms[0]]\n",
    "            self.W = opt_choice[0]\n",
    "            self.b = opt_choice[1]\n",
    "            latest_optimum = opt_choice[0][0]+step*2\n",
    "        \n",
    "    \n",
    "    '''\n",
    "    THIS METHOD USE THE SVM TRAINED AND PREDICT THE NEW DATA ABSE ON THE HYPERPLANES DEFINED\n",
    "    '''\n",
    "    def predict(self, features):\n",
    "        \n",
    "        ## CHECKING THE SIGN\n",
    "        classification = np.sign(np.dot(np.array(features), self.W)+self.b)\n",
    "        \n",
    "        ##\n",
    "        if classification != 0 and self.visualization:\n",
    "            self.ax.scatter(features[0], features[1],marker='*', s=200, c=self.colors[classification])\n",
    "        \n",
    "        ##\n",
    "        return classification\n",
    "    \n",
    "    '''\n",
    "    THIS METHOD IS USE TO VISUALIZE THE DATA CLASSIFID BY THE SVM PREDICT METHOD\n",
    "    '''\n",
    "    def visualize(self):\n",
    "        ##\n",
    "        [[self.ax.scatter(x[0],x[1],s=100,c=self.colors[i]) for x in data_dict[i]] for i in data_dict]\n",
    "        \n",
    "        ##\n",
    "        def hyperplane(x,w,b,v):\n",
    "            ##\n",
    "            return (-w[0]*x+v-b)/w[1]\n",
    "        \n",
    "        ## LIMITES FOR THE PLOT\n",
    "        datarange = (self.min_features_val*0.9, self.max_features_val*1.1)\n",
    "        hyper_x_min = datarange[0]\n",
    "        hyper_x_max = datarange[1]\n",
    "        \n",
    "        ## POSITIVE SUPORT HYPERPLANE (PSH)\n",
    "        ## (W*x+b)=1\n",
    "        psh_1 = hyperplane(hyper_x_min, self.W, self.b, 1)\n",
    "        psh_2 = hyperplane(hyper_x_max, self.W, self.b, 1)\n",
    "        \n",
    "        ## PLOTING THE HYPERPLANE\n",
    "        self.ax.plot([hyper_x_min, hyper_x_max], [psh_1,psh_2], 'k')\n",
    "        \n",
    "        ## NEGATIVE SUPORT HYPERPLANE (NSH)\n",
    "        ## (W*x+b)=-1\n",
    "        nsh_1 = hyperplane(hyper_x_min, self.W, self.b, -1)\n",
    "        nsh_2 = hyperplane(hyper_x_max, self.W, self.b, -1)\n",
    "        \n",
    "        ## PLOTING THE HYPERPLANE\n",
    "        self.ax.plot([hyper_x_min, hyper_x_max], [nsh_1,nsh_2], 'k')\n",
    "        \n",
    "        ## DECISION BONDARY HYPERPLANE (DBH)\n",
    "        ## (W*x+b)=1\n",
    "        dbh_1 = hyperplane(hyper_x_min, self.W, self.b, 0)\n",
    "        dbh_2 = hyperplane(hyper_x_max, self.W, self.b, 0)\n",
    "        \n",
    "        ## PLOTING THE HYPERPLANE\n",
    "        self.ax.plot([hyper_x_min, hyper_x_max], [dbh_1,dbh_2],'y--')\n",
    "        \n",
    "        ## PLOTING EVERYTHING\n",
    "        plt.show()\n",
    "        \n",
    "## END OF THE CLASS        \n",
    "############################################################################################       \n",
    "        \n",
    "## TEST        \n",
    "data_dict = {-1:np.array([[1,7], [2,8], [3,8],]), 1:np.array([[5,1], [6,-1], [7,3],])}\n",
    "\n",
    "\n",
    "## EXAMPLE\n",
    "hypo = Support_Vector_Machine()\n",
    "hypo.train(data_dict)\n",
    "\n",
    "\n",
    "## PREDICTION TEST\n",
    "predict_us = [[0,10], [1,3], [3,4], [3,5], [5,5], [5,6], [6,-5], [5,8], [3,2], [1,1], [2,2]]\n",
    "\n",
    "for i in predict_us:\n",
    "    hypo.predict(i)\n",
    "\n",
    "hypo.visualize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x4TpjzU7-k3L",
    "outputId": "1fd4cd44-a5fb-4c6b-a1e5-f0f9cda78fa6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.4143e+01 -2.8316e+01  5e+02  2e+01  2e+00\n",
      " 1: -1.9106e+01 -1.6341e+01  2e+02  8e+00  7e-01\n",
      " 2: -1.9269e+01 -7.6497e+00  7e+01  2e+00  2e-01\n",
      " 3: -6.4108e+00 -4.2617e+00  9e+00  3e-01  2e-02\n",
      " 4: -3.6827e+00 -3.8382e+00  2e-01  1e-04  1e-05\n",
      " 5: -3.8036e+00 -3.8052e+00  2e-03  1e-06  1e-07\n",
      " 6: -3.8048e+00 -3.8048e+00  2e-05  1e-08  1e-09\n",
      " 7: -3.8048e+00 -3.8048e+00  2e-07  1e-10  1e-11\n",
      "Optimal solution found.\n",
      "3 support vectors out of 180 points\n",
      "19 out of 20 predictions correct\n"
     ]
    }
   ],
   "source": [
    "# Mathieu Blondel, September 2010\n",
    "# License: BSD 3 clause\n",
    "# http://www.mblondel.org/journal/2010/09/19/support-vector-machines-in-python/\n",
    "\n",
    "# visualizing what translating to another dimension does\n",
    "# and bringing back to 2D:\n",
    "# https://www.youtube.com/watch?v=3liCbRZPrZA\n",
    "\n",
    "# Docs: http://cvxopt.org/userguide/coneprog.html#quadratic-programming\n",
    "# Docs qp example: http://cvxopt.org/examples/tutorial/qp.html\n",
    "\n",
    "# Nice tutorial:\n",
    "# https://courses.csail.mit.edu/6.867/wiki/images/a/a7/Qp-cvxopt.pdf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from numpy import linalg\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "             \n",
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p=3):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=5.0):\n",
    "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
    "\n",
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=linear_kernel, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: self.C = float(self.C)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = self.kernel(X[i], X[j])\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.a = a[sv]\n",
    "        self.sv = X[sv]\n",
    "        self.sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.a)):\n",
    "            self.b += self.sv_y[n]\n",
    "            self.b -= np.sum(self.a * self.sv_y * K[ind[n],sv])\n",
    "        self.b /= len(self.a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == linear_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.a)):\n",
    "                self.w += self.a[n] * self.sv_y[n] * self.sv[n]\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def project(self, X):\n",
    "        if self.w is not None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.a, self.sv_y, self.sv):\n",
    "                    s += a * sv_y * self.kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.project(X))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import pylab as pl\n",
    "\n",
    "    def gen_lin_separable_data():\n",
    "        # generate training data in the 2-d case\n",
    "        mean1 = np.array([0, 2])\n",
    "        mean2 = np.array([2, 0])\n",
    "        cov = np.array([[0.8, 0.6], [0.6, 0.8]])\n",
    "        X1 = np.random.multivariate_normal(mean1, cov, 100)\n",
    "        y1 = np.ones(len(X1))\n",
    "        X2 = np.random.multivariate_normal(mean2, cov, 100)\n",
    "        y2 = np.ones(len(X2)) * -1\n",
    "        return X1, y1, X2, y2\n",
    "\n",
    "    def gen_non_lin_separable_data():\n",
    "        mean1 = [-1, 2]\n",
    "        mean2 = [1, -1]\n",
    "        mean3 = [4, -4]\n",
    "        mean4 = [-4, 4]\n",
    "        cov = [[1.0,0.8], [0.8, 1.0]]\n",
    "        X1 = np.random.multivariate_normal(mean1, cov, 50)\n",
    "        X1 = np.vstack((X1, np.random.multivariate_normal(mean3, cov, 50)))\n",
    "        y1 = np.ones(len(X1))\n",
    "        X2 = np.random.multivariate_normal(mean2, cov, 50)\n",
    "        X2 = np.vstack((X2, np.random.multivariate_normal(mean4, cov, 50)))\n",
    "        y2 = np.ones(len(X2)) * -1\n",
    "        return X1, y1, X2, y2\n",
    "\n",
    "    def gen_lin_separable_overlap_data():\n",
    "        # generate training data in the 2-d case\n",
    "        mean1 = np.array([0, 2])\n",
    "        mean2 = np.array([2, 0])\n",
    "        cov = np.array([[1.5, 1.0], [1.0, 1.5]])\n",
    "        X1 = np.random.multivariate_normal(mean1, cov, 100)\n",
    "        y1 = np.ones(len(X1))\n",
    "        X2 = np.random.multivariate_normal(mean2, cov, 100)\n",
    "        y2 = np.ones(len(X2)) * -1\n",
    "        return X1, y1, X2, y2\n",
    "\n",
    "    def split_train(X1, y1, X2, y2):\n",
    "        X1_train = X1[:90]\n",
    "        y1_train = y1[:90]\n",
    "        X2_train = X2[:90]\n",
    "        y2_train = y2[:90]\n",
    "        X_train = np.vstack((X1_train, X2_train))\n",
    "        y_train = np.hstack((y1_train, y2_train))\n",
    "        return X_train, y_train\n",
    "\n",
    "    def split_test(X1, y1, X2, y2):\n",
    "        X1_test = X1[90:]\n",
    "        y1_test = y1[90:]\n",
    "        X2_test = X2[90:]\n",
    "        y2_test = y2[90:]\n",
    "        X_test = np.vstack((X1_test, X2_test))\n",
    "        y_test = np.hstack((y1_test, y2_test))\n",
    "        return X_test, y_test\n",
    "\n",
    "    def plot_margin(X1_train, X2_train, clf):\n",
    "        def f(x, w, b, c=0):\n",
    "            # given x, return y such that [x,y] in on the line\n",
    "            # w.x + b = c\n",
    "            return (-w[0] * x - b + c) / w[1]\n",
    "\n",
    "        pl.plot(X1_train[:,0], X1_train[:,1], \"ro\")\n",
    "        pl.plot(X2_train[:,0], X2_train[:,1], \"bo\")\n",
    "        pl.scatter(clf.sv[:,0], clf.sv[:,1], s=100, c=\"g\")\n",
    "\n",
    "        # w.x + b = 0\n",
    "        a0 = -4; a1 = f(a0, clf.w, clf.b)\n",
    "        b0 = 4; b1 = f(b0, clf.w, clf.b)\n",
    "        pl.plot([a0,b0], [a1,b1], \"k\")\n",
    "\n",
    "        # w.x + b = 1\n",
    "        a0 = -4; a1 = f(a0, clf.w, clf.b, 1)\n",
    "        b0 = 4; b1 = f(b0, clf.w, clf.b, 1)\n",
    "        pl.plot([a0,b0], [a1,b1], \"k--\")\n",
    "\n",
    "        # w.x + b = -1\n",
    "        a0 = -4; a1 = f(a0, clf.w, clf.b, -1)\n",
    "        b0 = 4; b1 = f(b0, clf.w, clf.b, -1)\n",
    "        pl.plot([a0,b0], [a1,b1], \"k--\")\n",
    "\n",
    "        pl.axis(\"tight\")\n",
    "        pl.show()\n",
    "\n",
    "    def plot_contour(X1_train, X2_train, clf):\n",
    "        pl.plot(X1_train[:,0], X1_train[:,1], \"ro\")\n",
    "        pl.plot(X2_train[:,0], X2_train[:,1], \"bo\")\n",
    "        pl.scatter(clf.sv[:,0], clf.sv[:,1], s=100, c=\"g\")\n",
    "\n",
    "        X1, X2 = np.meshgrid(np.linspace(-6,6,50), np.linspace(-6,6,50))\n",
    "        X = np.array([[x1, x2] for x1, x2 in zip(np.ravel(X1), np.ravel(X2))])\n",
    "        Z = clf.project(X).reshape(X1.shape)\n",
    "        pl.contour(X1, X2, Z, [0.0], colors='k', linewidths=1, origin='lower')\n",
    "        pl.contour(X1, X2, Z + 1, [0.0], colors='grey', linewidths=1, origin='lower')\n",
    "        pl.contour(X1, X2, Z - 1, [0.0], colors='grey', linewidths=1, origin='lower')\n",
    "\n",
    "        pl.axis(\"tight\")\n",
    "        pl.show()\n",
    "\n",
    "    def test_linear():\n",
    "        X1, y1, X2, y2 = gen_lin_separable_data()\n",
    "        X_train, y_train = split_train(X1, y1, X2, y2)\n",
    "        X_test, y_test = split_test(X1, y1, X2, y2)\n",
    "\n",
    "        clf = SVM()\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_predict = clf.predict(X_test)\n",
    "        correct = np.sum(y_predict == y_test)\n",
    "        print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "\n",
    "        plot_margin(X_train[y_train==1], X_train[y_train==-1], clf)\n",
    "\n",
    "    def test_non_linear():\n",
    "        X1, y1, X2, y2 = gen_non_lin_separable_data()\n",
    "        X_train, y_train = split_train(X1, y1, X2, y2)\n",
    "        X_test, y_test = split_test(X1, y1, X2, y2)\n",
    "\n",
    "        clf = SVM(gaussian_kernel)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_predict = clf.predict(X_test)\n",
    "        correct = np.sum(y_predict == y_test)\n",
    "        print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "\n",
    "        plot_contour(X_train[y_train==1], X_train[y_train==-1], clf)\n",
    "\n",
    "    def test_soft():\n",
    "        X1, y1, X2, y2 = gen_lin_separable_overlap_data()\n",
    "        X_train, y_train = split_train(X1, y1, X2, y2)\n",
    "        X_test, y_test = split_test(X1, y1, X2, y2)\n",
    "\n",
    "        clf = SVM(C=1000.1)\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        y_predict = clf.predict(X_test)\n",
    "        correct = np.sum(y_predict == y_test)\n",
    "        print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "\n",
    "        plot_contour(X_train[y_train==1], X_train[y_train==-1], clf)\n",
    "\n",
    "        \n",
    "    test_linear()\n",
    "    #test_non_linear()\n",
    "    #test_soft()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyZtgeka-k3P"
   },
   "source": [
    "# **UNSUPERVISED LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bgpRLLca-k3S",
    "outputId": "1f606331-f956-4334-fe73-d754c8dacf90"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guilhermelinux/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/guilhermelinux/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/guilhermelinux/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/guilhermelinux/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9522226400>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "################################## K-MEANS #################################################\n",
    "## IMPORTING lIBRARIES\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## DEFINING THE STYLE\n",
    "style.use('ggplot')\n",
    "\n",
    "## DEFINING THE DATA SET\n",
    "X = np.array([[1,3],[0.7,0.9],[3,1],[6,5],[4,7],[6,6],[3,3]])\n",
    "\n",
    "## PREVIEWING THE DATA\n",
    "#plt.scatter(X[:, 0],X[:, 1], s=150, linewidths = 5, zorder = 10)\n",
    "#plt.show()\n",
    "\n",
    "## DEFINING THE NUMBER OF CLUSTERS\n",
    "hypo = KMeans(n_clusters=2)\n",
    "\n",
    "## FITING THE HYPOTHESIS\n",
    "hypo.fit(X)\n",
    "\n",
    "## PREDICTION ARRAY\n",
    "predict_us = np.array([[2,4],[4,4],[3,10],[10,1],[8,8],[10,6]])\n",
    "\n",
    "## TESTING THE PREDICTION\n",
    "hypo.predict(predict_us)\n",
    "\n",
    "## GETING THE CENTROID POSITION AND THE LABELS\n",
    "means_centroids = hypo.cluster_centers_\n",
    "labels = hypo.labels_\n",
    "\n",
    "## PLOTING THE CLUSTERING RESULT\n",
    "colors = ['g.', 'r.', 'b.']\n",
    "## PLOTING THE TRAINING DATA\n",
    "[plt.plot(X[i][0], X[i][1], colors[labels[i]], markersize = 10) for i in range(len(X))]\n",
    "## PLOTING THE PREDICT DATA\n",
    "[plt.plot(predict_us[i][0], predict_us[i][1], colors[labels[i]], marker = '*', markersize = 10) for i in range(len(predict_us))] \n",
    "## PLOTING THE CENTROID\n",
    "plt.scatter(means_centroids[:,0], means_centroids[:,1], marker = 'x', s=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HT1tjdWf-k3X",
    "outputId": "e4e99282-026c-47b3-8de9-55a5b6e3b908"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'titanic.xls'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-17f98122e8ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m## EXTRACTING THE DATASET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'titanic.xls'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;31m## PREVIEW THE DATA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m#print df.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    176\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_deprecate_kwarg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, **kwds)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     return io.parse(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/io/excel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, io, **kwds)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxlrd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             raise ValueError('Must explicitly set engine if not passing in'\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'titanic.xls'"
     ]
    }
   ],
   "source": [
    "################################### TITANIC KMEANS #########################################\n",
    "## IMPORTING THE LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import sklearn.preprocessing\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "## DEFINING THE STYLE\n",
    "style.use('ggplot')\n",
    "\n",
    "## FUNCTIONS\n",
    "\n",
    "'''\n",
    "THIS FUNCTION CONVERT THE NON NUMERICAL DATA PRESENT I THE DATA SET TO A NUMERICAL DATA\n",
    "THIS FUNCTION TAKES AS A INPUT THE DATAFRAME THAT NEEDS TO BE CHANGE AND RETURN THE SAME\n",
    "DATAFRAME AFTER THE CONVERTION\n",
    "\n",
    "'''\n",
    "def convert_non_numerical_data(df):\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    ## CHECK FOR EACH COLUMS IN THE DATAFRAME\n",
    "    for column in columns:\n",
    "        test_digt_value = {}\n",
    "        \n",
    "        ## THIS FUNCTION IS RESPONSIBLE TO ACTUALY CONVERT THE DATA TO A INT\n",
    "        def convert_to_int(value):\n",
    "            return test_digt_value[value]\n",
    "        \n",
    "        ## CKECK TO SEE IF THE DATAPOINT NON NUMERICAL\n",
    "        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n",
    "            column_contents = df[column].values.tolist()\n",
    "            unique_elements = set(column_contents)\n",
    "            ## GET THE UNIQUE VALUES \n",
    "            x=0\n",
    "            for unique in unique_elements:\n",
    "                if unique is not test_digt_value:\n",
    "                    test_digt_value[unique] = x\n",
    "                    x += 1\n",
    "            ##MAP THE VALUES INTO TEH COLUMN\n",
    "            df[column] = list(map(convert_to_int, df[column]))\n",
    "            \n",
    "    ## RETURN THE DATAFRAME AFTER CONVERSION\n",
    "    return df\n",
    "\n",
    "## EXTRACTING THE DATASET\n",
    "df = pd.read_excel('titanic.xls')\n",
    "## PREVIEW THE DATA\n",
    "#print df.head()\n",
    "\n",
    "## WRANGILING THE DATA\n",
    "df.drop(['body', 'name'], 1, inplace = True)\n",
    "df.fillna(0, inplace = True)\n",
    "df = convert_non_numerical_data(df)\n",
    "## PREVIEW THE DATA\n",
    "#print df.head()\n",
    "\n",
    "## DEFINING THE FEATURE SET\n",
    "X = np.array(df.drop(['survived',], 1)).astype(float)\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "## DEFINING THE LABELS SET...TO SEE THE ACCURACY\n",
    "y = np.array(df['survived'])\n",
    "\n",
    "## DEFINING THE HYPOTHESIS AND TRAINING IT\n",
    "hypo = KMeans(n_clusters = 2)   ##SURVIVED??\n",
    "hypo.fit(X)\n",
    "\n",
    "## TESTING ACCURACY\n",
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "    predict_us = np.array(X[i].astype(float))\n",
    "    predict_us = predict_us.reshape(-1, len(predict_us))\n",
    "    prediction = hypo.predict(predict_us)\n",
    "    if prediction == y[i]:\n",
    "        correct +=1\n",
    "print('THE ACCUCARY IS {}%').format(100*float(correct)/float(len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTXsP9nw-k3d",
    "outputId": "37703948-8569-4b93-82b7-ac1cc61cc3ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE ACCUCARY IS 69.1367456073%\n"
     ]
    }
   ],
   "source": [
    "################################# K-MEANS FROM SCRATCH #####################################\n",
    "## IMPORTING lIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import sklearn.preprocessing\n",
    "\n",
    "## DEFINING THE STYLE\n",
    "style.use('ggplot')\n",
    "\n",
    "## DEFING THE CLASS FOR THE K-MEANS CLASSIFIER\n",
    "class K_Means:\n",
    "    ## CONSTRUCTOR\n",
    "    def __init__(self, k_cluster = 2, tol = 0.001, max_iter = 300):\n",
    "        ## DEFINING THE ATRIBUTES\n",
    "        self.k_cluster = k_cluster\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    ## CREATING THE METHODS\n",
    "    '''\n",
    "    THIS METHOD FINDS THE CLUSTERS PRESENT ON OUR DATA, THE CLUSTERS ARE COMPUTE BASE ON\n",
    "    THE K-MEANS ALGORITHM.\n",
    "    FOR THE FIRST ITERATION THE CENTROIDS ARE THE Kth FISRTS ELEMENTS ON THE DATA\n",
    "    FOR LATER ITERATION THE CENTROIDS MOVE BASE ON THE MEAN OF THE DATAPOINTS ON THE CLUSTER\n",
    "    THE ALGORITHM CONVERGES WHEN THE CENTROIDS STOPS MOVING BETWEEN THE TOLERATION\n",
    "    \n",
    "    self -> CLASS REFERENCE FOR ATRIBUTES\n",
    "    data -> TRAINING DATA\n",
    "    '''\n",
    "    def fit(self, data):\n",
    "        ## DEFINING THE ATRIBUTES\n",
    "        self.centroids = {}\n",
    "        \n",
    "        ## DEFINING THE POSITION OF THE CENTROIDS FOR THE FIRST ITERATION\n",
    "        for i in range(self.k_cluster):\n",
    "            self.centroids[i]  = data[i]   ##THE POSITION ARE THE FIRST Ith ELEMENTS\n",
    "        \n",
    "        ## START THE CLASSIFICATION PROCESS\n",
    "        for i in range(self.max_iter):\n",
    "            self.classifications = {}\n",
    "            \n",
    "            ## CLASSIFICATING FOR EACH K CENTROID\n",
    "            for i in range(self.k_cluster):\n",
    "                self.classifications[i] = []   ##CLEANING FOR EACH CENTROID\n",
    "                \n",
    "            ## COMPUTING THE DISTACE OF THE FEATURESET FOR EACH CENTROID\n",
    "            for featureset in data:\n",
    "                dist = [np.linalg.norm(featureset-self.centroids[centroid])for centroid in self.centroids]\n",
    "                classification = dist.index(min(dist))   ##TAKING THE CLOSEST ONE\n",
    "                self.classifications[classification].append(featureset)\n",
    "                \n",
    "            ## SAVING THE VALUE OF THE CENTROIDS FOR FOWARD COMPARATION\n",
    "            prev_centroids = dict(self.centroids)\n",
    "                \n",
    "            ## TAKING THE MEAN OF THE CLUSTERS AND MOVE THE CENTROIDS \n",
    "            for classification in self.classifications:\n",
    "                self.centroids[classification] = np.average(self.classifications[classification], axis = 0)\n",
    "                            \n",
    "            ## VERIFING IF THE CLUSTERS ARE FOUND \n",
    "            clusters_found = True\n",
    "                \n",
    "            ## CHECKING THE TOLERANCE\n",
    "            for cent in self.centroids:\n",
    "                previous_centroid = prev_centroids[cent]\n",
    "                current_centroid = self.centroids[cent]\n",
    "                if float(np.sum((current_centroid-previous_centroid)/(previous_centroid*100.0))) > self.tol:   ## TOL IS IN %\n",
    "                    clusters_found = False   ##NOT FOUNDED!!!\n",
    "                \n",
    "            ## WE FOUNDED!!\n",
    "            if clusters_found:\n",
    "                    break\n",
    "                    \n",
    "    \n",
    "    '''\n",
    "    THIS METHOD USE THE CENTROIDS POSITION COMPUTED ON THE fit METHOD TO FIND THE CLUSTERS \n",
    "    FOR THE PREDICTION DATA\n",
    "    \n",
    "    self -> CLASS REFERENCE FOR ATRIBUTES\n",
    "    data -> PREDICTION DATA\n",
    "    '''\n",
    "    def predict(self, data):\n",
    "        ## COMPUTING THE DISTACE OF THE DATA FOR EACH CENTROID\n",
    "        dist = [np.linalg.norm(data-self.centroids[centroid])\n",
    "                            for centroid in self.centroids]\n",
    "        classification = dist.index(min(dist))   ##TAKING THE CLOSEST ONE\n",
    "        return classification\n",
    "    \n",
    "\n",
    "## FUNCTIONS\n",
    "\n",
    "'''\n",
    "THIS FUNCTION CONVERT THE NON NUMERICAL DATA PRESENT I THE DATA SET TO A NUMERICAL DATA\n",
    "THIS FUNCTION TAKES AS A INPUT THE DATAFRAME THAT NEEDS TO BE CHANGE AND RETURN THE SAME\n",
    "DATAFRAME AFTER THE CONVERTION\n",
    "\n",
    "'''\n",
    "def convert_non_numerical_data(df):\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    ## CHECK FOR EACH COLUMS IN THE DATAFRAME\n",
    "    for column in columns:\n",
    "        test_digt_value = {}\n",
    "        \n",
    "        ## THIS FUNCTION IS RESPONSIBLE TO ACTUALY CONVERT THE DATA TO A INT\n",
    "        def convert_to_int(value):\n",
    "            return test_digt_value[value]\n",
    "        \n",
    "        ## CKECK TO SEE IF THE DATAPOINT NON NUMERICAL\n",
    "        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n",
    "            column_contents = df[column].values.tolist()\n",
    "            unique_elements = set(column_contents)\n",
    "            ## GET THE UNIQUE VALUES \n",
    "            x=0\n",
    "            for unique in unique_elements:\n",
    "                if unique is not test_digt_value:\n",
    "                    test_digt_value[unique] = x\n",
    "                    x += 1\n",
    "            ##MAP THE VALUES INTO TEH COLUMN\n",
    "            df[column] = list(map(convert_to_int, df[column]))\n",
    "            \n",
    "    ## RETURN THE DATAFRAME AFTER CONVERSION\n",
    "    return df\n",
    "\n",
    "## EXTRACTING THE DATASET\n",
    "df = pd.read_excel('titanic.xls')\n",
    "## PREVIEW THE DATA\n",
    "#print df.head()\n",
    "\n",
    "## WRANGILING THE DATA\n",
    "df.drop(['body', 'name'], 1, inplace = True)\n",
    "df.fillna(0, inplace = True)\n",
    "df = convert_non_numerical_data(df)\n",
    "## PREVIEW THE DATA\n",
    "#print df.head()\n",
    "\n",
    "## DEFINING THE FEATURE SET\n",
    "X = np.array(df.drop(['survived',], 1)).astype(float)\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "## DEFINING THE LABELS SET...TO SEE THE ACCURACY\n",
    "y = np.array(df['survived'])\n",
    "\n",
    "\n",
    "## DEFINING THE HYPOTHESIS AND TRAINING IT\n",
    "hypo = K_Means()\n",
    "hypo.fit(X)\n",
    "\n",
    "## TESTING ACCURACY\n",
    "correct = 0\n",
    "for i in range(len(X)):\n",
    "    predict_us = np.array(X[i].astype(float))\n",
    "    predict_us = predict_us.reshape(-1, len(predict_us))\n",
    "    prediction = hypo.predict(predict_us)\n",
    "    if prediction == y[i]:\n",
    "        correct +=1\n",
    "print('THE ACCUCARY IS {}%').format(100*float(correct)/float(len(X)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSDgpOtp-k3l",
    "outputId": "976db7ad-2c9e-4f90-88d7-eed81381a098"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE NUMBER OF CLUSTERS FINDS BY MEAN SHIFT IS  3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<mpl_toolkits.mplot3d.art3d.Path3DCollection at 0x7fbac96a93d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##################################### MEANS SHIFT ##########################################\n",
    "## IMPORTING lIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import style\n",
    "import sklearn.preprocessing\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "## DEFINING THE STYLE\n",
    "style.use('ggplot')\n",
    "\n",
    "## FIRST ITERATION CENTROIDS\n",
    "centers = [[1,1,1],[5,5,5],[3,10,10]]\n",
    "\n",
    "## CREATING THE TRAINIG DATA\n",
    "X, _ = make_blobs(n_samples = 100, centers = centers, cluster_std = 1.5)\n",
    "\n",
    "## DEFINING MEANS SHIFT HYPOTHESIS AND TRAINING\n",
    "hypo = MeanShift()\n",
    "hypo.fit(X)\n",
    "\n",
    "## GETING THE CENTROID POSITION AND THE LABELS\n",
    "cluster_centroids = hypo.cluster_centers_\n",
    "labels = hypo.labels_\n",
    "\n",
    "## PRINTING THE NUMBER OF CLUSTERS\n",
    "n_cluster_ = len(np.unique(labels))\n",
    "print 'THE NUMBER OF CLUSTERS FINDS BY MEAN SHIFT IS ',n_cluster_\n",
    "\n",
    "## DEFINING THE 3D SPACE\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection = '3d')\n",
    "\n",
    "## PLOTING THE DATA IN THE 3D SPACE\n",
    "colors = 10*['r','g','b','c','k','y','m']\n",
    "for i in range(len(X)):\n",
    "    ax.scatter(X[i][0], X[i][1], X[i][2], c = colors[labels[i]], marker = 'o')\n",
    "    \n",
    "## PRINTING THE CENTROIDS CLUSTERS\n",
    "ax.scatter(cluster_centroids[:,0], cluster_centroids[:,1], cluster_centroids[:, 2], marker = 'x', color = 'k', s = 150, linewidths = 5, zorder = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OIzLXRXp-k3r",
    "outputId": "da67489a-13ed-442d-8fdd-8237011583d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.37782982045277125, 1: 0.8333333333333334, 2: 0.1}\n"
     ]
    }
   ],
   "source": [
    "################################ TITANIC MEANSHIFT #########################################\n",
    "## IMPORTING THE LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import sklearn.preprocessing\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "## FUNCTIONS\n",
    "\n",
    "'''\n",
    "THIS FUNCTION CONVERT THE NON NUMERICAL DATA PRESENT I THE DATA SET TO A NUMERICAL DATA\n",
    "THIS FUNCTION TAKES AS A INPUT THE DATAFRAME THAT NEEDS TO BE CHANGE AND RETURN THE SAME\n",
    "DATAFRAME AFTER THE CONVERTION\n",
    "\n",
    "'''\n",
    "def convert_non_numerical_data(df):\n",
    "    columns = df.columns.values\n",
    "    \n",
    "    ## CHECK FOR EACH COLUMS IN THE DATAFRAME\n",
    "    for column in columns:\n",
    "        test_digt_value = {}\n",
    "        \n",
    "        ## THIS FUNCTION IS RESPONSIBLE TO ACTUALY CONVERT THE DATA TO A INT\n",
    "        def convert_to_int(value):\n",
    "            return test_digt_value[value]\n",
    "        \n",
    "        ## CKECK TO SEE IF THE DATAPOINT NON NUMERICAL\n",
    "        if df[column].dtype != np.int64 and df[column].dtype != np.float64:\n",
    "            column_contents = df[column].values.tolist()\n",
    "            unique_elements = set(column_contents)\n",
    "            ## GET THE UNIQUE VALUES \n",
    "            x=0\n",
    "            for unique in unique_elements:\n",
    "                if unique is not test_digt_value:\n",
    "                    test_digt_value[unique] = x\n",
    "                    x += 1\n",
    "            ##MAP THE VALUES INTO TEH COLUMN\n",
    "            df[column] = list(map(convert_to_int, df[column]))\n",
    "            \n",
    "    ## RETURN THE DATAFRAME AFTER CONVERSION\n",
    "    return df\n",
    "\n",
    "## EXTRACTING THE DATASET\n",
    "df = pd.read_excel('titanic.xls')\n",
    "## PREVIEW THE DATA\n",
    "#print df.head()\n",
    "\n",
    "##MAKING A SECURITY COPY OF THE ORIGINAL DATASET\n",
    "original_df = pd.DataFrame.copy(df)\n",
    "\n",
    "## WRANGILING THE DATA\n",
    "df.drop(['body', 'name'], 1, inplace = True)\n",
    "df.fillna(0, inplace = True)\n",
    "df = convert_non_numerical_data(df)\n",
    "## PREVIEW THE DATA\n",
    "#print df.head()\n",
    "\n",
    "## DEFINING THE FEATURE SET\n",
    "X = np.array(df.drop(['survived',], 1)).astype(float)\n",
    "X = sklearn.preprocessing.scale(X)\n",
    "## DEFINING THE LABELS SET...TO SEE THE ACCURACY\n",
    "y = np.array(df['survived'])\n",
    "\n",
    "## DEFINING THE HYPOTHESIS AND TRAINING IT\n",
    "hypo = MeanShift()   ##FINDING THE CLUSTERS\n",
    "hypo.fit(X)\n",
    "\n",
    "## GETING THE CENTROID POSITION, LABELS AND NUMBER OS CLUSTERS\n",
    "cluster_centroids = hypo.cluster_centers_\n",
    "labels = hypo.labels_\n",
    "n_clusters = np.unique(labels)\n",
    "\n",
    "## REMOVE THE WARNNIG MENSAGE OF CHANGING A COPY DATAFRAME \n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "## CREATING THE CLUSTER COLUMN AT THE DATASET\n",
    "original_df['cluster_group'] = np.nan\n",
    "for i in range(len(X)):\n",
    "    original_df['cluster_group'].iloc[i] = labels[i]\n",
    "\n",
    "## FIDING THE SURVIVAL RATES FOR EACH CLUSTER\n",
    "survival_rates = {}\n",
    "for i in range(len(cluster_centroids)):\n",
    "    aux_df = original_df[(original_df['cluster_group'] == float(i))]\n",
    "    survival_in_cluster = aux_df[(aux_df['survived'] == 1)]\n",
    "    survival_rate = float(len(survival_in_cluster))/float(len(aux_df))\n",
    "    survival_rates[i] = survival_rate\n",
    "    \n",
    "print (survival_rates)\n",
    "\n",
    "## PLAYING WUTH THE DATA...****KEEP IN MIND THAT THE RESULTS MAY VARY DUE TO THE MEANSHITH ALGORITHM PROPERTIES\n",
    "\n",
    "#print(original_df[(original_df['cluster_group'] == 0)].describe())\n",
    "#print(original_df[(original_df['cluster_group'] == 1)].describe())\n",
    "#print(original_df[(original_df['cluster_group'] == 2)].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tl2kyO6I-k3x",
    "outputId": "caccc8c1-2518-4006-d2c2-28cffbbb4df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############################### MEANSHIFT FROM SCRATCH #####################################\n",
    "## IMPORTING lIBRARIES\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "\n",
    "## DEFINING THE STYLE\n",
    "style.use('ggplot')\n",
    "\n",
    "## DEFINING THE CLASS FOR THE MEANSHIFT CLASSIFIER\n",
    "class Mean_Shift:\n",
    "    \n",
    "    ## CONSTRUCTOR METHOD\n",
    "    def __init__(self, radius = None, bandwidth_size = 100):\n",
    "        ## DEFINING THE ATRIBUTE\n",
    "        self.radius = radius\n",
    "        self.bandwidth_size = bandwidth_size\n",
    "    \n",
    "    ## CREATING THE METHODS\n",
    "    '''\n",
    "    THIS METHODS CALCULATE THE RADIUS AND THE WEIGTHS. THE RADIUS IS THE DIVISION OF THE ALL \n",
    "    DATA CENTROID BY THE BANDWIDTH SIZE. THE WEIGTHS ARE THE INVERSE OF THE BANDWIDTH SIZE,\n",
    "    POINTS CLOSE TO THE CENTROIDS HAVE LESS WEIGTH.\n",
    "    \n",
    "    self -> CLASS REFERENCE FOR ATRIBUTES\n",
    "    data -> PREDICTION DATA\n",
    "    '''\n",
    "    def radius_and_weigths(self, data):\n",
    "        ## FINDING THE CENTROID OF ALL DATASET\n",
    "        all_data_centroid = np.average(data)\n",
    "        all_data_dist = np.linalg.norm(all_data_centroid)\n",
    "        self.radius = float(all_data_dist)/float(self.bandwidth_size)\n",
    "        ## DEFINIG THE WEIGTHS\n",
    "        weigths = [i for i in range(self.bandwidth_size)][: : -1]\n",
    "        return weigths \n",
    "    '''\n",
    "    THIS METHOD FINDS THE CLUSTERS PRESENT ON OUR DATA, THE CLUSTERS ARE COMPUTE BASE ON\n",
    "    THE MEANSSHIFT ALGORITHM.\n",
    "    THE RADIUS AND THE WEIGTH ARE DEFINE AUTOMATICALLY BY THE METHOD radius_and_weigths\n",
    "    THE ALGORITHM CONVERGE WHEM THE CENTROIDS STOPS MOVING\n",
    "    \n",
    "    self -> CLASS REFERENCE FOR ATRIBUTES\n",
    "    data -> TRAINING DATA\n",
    "    '''\n",
    "    def fit(self, data):\n",
    "        ## DEFINIG THE RADIUS SIZE AND WEIGTHS\n",
    "        if self.radius == None:\n",
    "            weigths = self.radius_and_weigths(data)\n",
    "        ## DEFINING THE CENTROIDS FOR THE FIRTS ITERATION\n",
    "        centroids = {}\n",
    "        for i in range(len(data)): ## EVERY POINT IS A CENTROID\n",
    "            centroids[i] = data[i] \n",
    "        \n",
    "        while True:   ## DO THIS LOOP UNTIL CONVERGION\n",
    "            \n",
    "            ## COMPUTING THE NEW CENTROIDS\n",
    "            n_centroids = []\n",
    "            for i in centroids: \n",
    "                in_radius = []\n",
    "                centroid = centroids[i]\n",
    "                for featureset in data:   ## FOR EACH DATAPOINT\n",
    "                    dist = np.linalg.norm(featureset - centroid)\n",
    "                    if dist == 0:   ## FOR THE FIRTS ITERATION\n",
    "                        dist = 0.000000001\n",
    "                    weigth_index = int(dist/self.radius)   ## RADIUS STEPS\n",
    "                    if weigth_index > self.bandwidth_size - 1:   ## ITS GREATHER THAN THE MAX DISTANCE?\n",
    "                        weigth_index = self.bandwidth_size - 1 \n",
    "                    ## THE FORMULA TO DEFINE THE RADIUS\n",
    "                    to_add = (weigths[weigth_index]**2)*[featureset]\n",
    "                    in_radius += to_add\n",
    "                    \n",
    "                ## THE NEW CENTROID IS THE AVARAGE OF THE POINTS INTHE RADIUS\n",
    "                n_centroid = np.average(in_radius, axis = 0)\n",
    "                n_centroids.append(tuple(n_centroid))\n",
    "                \n",
    "            ## TAKING THE UNIQUE CENTROIDS\n",
    "            unique = sorted(list(set(n_centroids)))\n",
    "            \n",
    "            ## CONVERGING THE CENTROIDS IN ONE RADIUS STEP\n",
    "            to_pop = []\n",
    "            for i in unique:\n",
    "                for ii in unique:\n",
    "                    if i == ii:\n",
    "                        pass\n",
    "                    elif np.linalg.norm(np.array(i) - np.array(ii)) <= self.radius:\n",
    "                        to_pop.append(ii)\n",
    "                        break\n",
    "            \n",
    "            ## REMOVING THE CENTROIDS IN ONE STEP RADIUS\n",
    "            for i in to_pop:\n",
    "                try:\n",
    "                    unique.remove(i)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            ## CHECKING IF IT HAS CONVERGE\n",
    "            pre_centroids = dict(centroids)\n",
    "            centroids =  {}\n",
    "            for i in range(len(unique)):\n",
    "                centroids[i] = np.array(unique[i])\n",
    "            optimized =True\n",
    "            for i in centroids:\n",
    "                if not np.array_equal(centroids[i], pre_centroids[i]): ## THEY ARE NOT EQUAL\n",
    "                    optimized = False\n",
    "                if not optimized:   ## ALL CENTROIDS NEEDS TO CONVERGE\n",
    "                    break\n",
    "                \n",
    "            ## THE ALGORITHM HAD CONVERGE?\n",
    "            if optimized:\n",
    "                break   ## YES!!!\n",
    "            \n",
    "        ## DEFINING THE ATRIBUTE\n",
    "        self.centroids = centroids\n",
    "        \n",
    "        ## DEFINING THE ATRIBUTE\n",
    "        self.classifications = {}\n",
    "        \n",
    "        ## CREATING THE CLASSIFICATIONS\n",
    "        for i in range(len(self.centroids)):\n",
    "            self.classifications[i] = []\n",
    "        \n",
    "        ## GETTING THE CLASSIFICATION FOR THE DATA\n",
    "        for featureset in data:\n",
    "            dist = [np.linalg.norm(featureset - self.centroids[centroid]) for centroid in self.centroids]\n",
    "            classification = dist.index(min(dist))\n",
    "            self.classifications[classification].append(featureset) \n",
    "    \n",
    "    '''\n",
    "    THIS METHOD USE THE CENTROIDS POSITION COMPUTED ON THE fit METHOD TO FIND THE CLUSTERS \n",
    "    FOR THE PREDICTION DATA\n",
    "    \n",
    "    self -> CLASS REFERENCE FOR ATRIBUTES\n",
    "    data -> PREDICTION DATA\n",
    "    '''\n",
    "    def predict(self, data):\n",
    "        dist = [np.linalg.norm(featureset - self.centroids[centroid]) for i in self.centroids]\n",
    "        classification = dist.index(min(dist))\n",
    "        return classification\n",
    "\n",
    "## DEFINING THE DATA SET\n",
    "## FOR FUN CHANGE THE DATASET TO SEE HOW THE ALGORITHM WORKS\n",
    "#X = np.array([[1,3],[0.7,0.9],[2,1],[6,5],[4,7],[6,6]])\n",
    "\n",
    "#X = np.array([[1, 2], [1.5, 1.8], [5, 8 ], [8, 8], [1, 0.6], [9,11], [8,2], [10,2], [9,3],])\n",
    "\n",
    "X, _ = make_blobs(n_samples = 15, centers = 3, n_features = 2)\n",
    "\n",
    "## DEFINING THE HYPOTHESYS\n",
    "hypo = Mean_Shift()\n",
    "hypo.fit(X)\n",
    "\n",
    "\n",
    "## PREVIEWING THE DATA\n",
    "colors = 10*['r','g','b','c','k','y','m']\n",
    "for classification in hypo.classifications:\n",
    "    color = colors[classification]\n",
    "    for featureset in hypo.classifications[classification]:\n",
    "        plt.scatter(featureset[0], featureset[1], color = color, s = 150)\n",
    "        \n",
    "## PREVIEWING THE CENTROIDS\n",
    "centroids = hypo.centroids\n",
    "print len(centroids)\n",
    "for c in hypo.centroids:\n",
    "    plt.scatter(centroids[c][0], centroids[c][1], color = 'k', marker = '*', s= 100, linewidths = 2, zorder = 10)\n",
    "\n",
    "\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cbJbH5QN-k36"
   },
   "source": [
    "# **NEURAL NETWORKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8cDggLbo-k38",
    "outputId": "85570aa1-1993-49bb-b67a-027fd0f537f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Mul_3:0\", shape=(), dtype=float32)\n",
      "8.0\n"
     ]
    }
   ],
   "source": [
    "## BASIC TENSORFLOW CODE.....DO NOT UPLOAD\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "x1 = tf.constant(2.0)\n",
    "x2 = tf.constant(4.0)\n",
    "\n",
    "ans = tf.multiply(x1, x2)\n",
    "\n",
    "print ans\n",
    "\n",
    "with tf.Session() as session:\n",
    "    print (session.run(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jAoB-pta-k4B",
    "outputId": "196118ba-be0e-4deb-a6b1-89733f1b4d00"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b59e3183fe03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m################################## MNIST DATASET ###########################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## IMPORT THE LIBRARIES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtutorials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "################################## MNIST DATASET ###########################################\n",
    "## IMPORT THE LIBRARIES\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "## READING THE DATASET\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot = True)   ## one_hot MAKE THIS A BINARY CLASSIFIER NOT A PROBABILISTC\n",
    "\n",
    "## DEFINING THE NEURAL NETWORK HYPERPARAMETERS\n",
    "\n",
    "## NUMBER OF NEURONS AT THE HIDDEN LAYERS...FEEL FREE TO CHANGE THIS\n",
    "n_nodes_hl0 = 784   ## THE LEN OF THE FEATURE SET\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "## NUMBER OF CLASSES\n",
    "n_classes = 10   ## ONE FOR EACH DIGIT 0->9\n",
    "\n",
    "## DEFINING THE BATCH SIZE\n",
    "batch_size = 100\n",
    "\n",
    "## DEFINING THE FEATURES SET\n",
    "X = tf.placeholder('float', [None, n_nodes_hl0])\n",
    "\n",
    "## DEFINING THE LABELS\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "THIS FUNCTION DEFINE THE MODEL FOR THE NN\n",
    "'''\n",
    "def neural_network_model(data):\n",
    "    \n",
    "    ## INICIALAZING THE WEIGTS AND BIAS FOR EACH LAYER\n",
    "    hidden_l1 = {'W': tf.Variable(tf.random_normal([n_nodes_hl0, n_nodes_hl1])), 'b': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    hidden_l2 = {'W': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), 'b': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    hidden_l3 = {'W': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), 'b': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    hidden_l4 = {'W': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), 'b': tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    ## CREATING THE NEURONS...z = X*W+b...a = g(z)...IN THIS CASE g() IS THE RELU\n",
    "    z1 = tf.add(tf.matmul(data, hidden_l1['W']), hidden_l1['b'])\n",
    "    a1 = tf.nn.relu(z1)\n",
    "    z2 = tf.add(tf.matmul(a1, hidden_l2['W']), hidden_l2['b'])\n",
    "    a2 = tf.nn.relu(z2)\n",
    "    z3 = tf.add(tf.matmul(a2, hidden_l3['W']), hidden_l3['b'])\n",
    "    a3 = tf.nn.relu(z3)\n",
    "    ## OUTPUT NEURON \n",
    "    output = tf.matmul(a3, hidden_l4['W']) + hidden_l4['b']\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "'''\n",
    "THIS FUNCTION TRAINS AND TEST THE NN DEFINED AT THE neural_network_model FUNCTION\n",
    "'''\n",
    "def train_test_neural_network(X):\n",
    "    ## TRAINING THE NN\n",
    "    \n",
    "    ## PREDICTION\n",
    "    y_hat = neural_network_model(X)\n",
    "    \n",
    "    ## DEFINING THE COST FUNCTION...WILL BE SOFTMAX 'CAUSE THE MULTI CLASSIFICATION\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = y_hat, labels = y))\n",
    "    \n",
    "    ## DEFINING THE OPTIMIZER \n",
    "    opt = tf.train.AdamOptimizer().minimize(cost)   ## BY DEFAULT alpha = 0.001\n",
    "    \n",
    "    ## DEFINING THE NUMBER OF ITERATIONS...FEEDFOWARD+BACKṔROP\n",
    "    hm_epoch = 10\n",
    "    \n",
    "    ## STARTING THE TENSORFLOW SESSION\n",
    "    with tf.Session() as sess:\n",
    "        ## INITIALIZING THE VARIABLES\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ## CALCULATING THE LOSS FOR EACH ITERATION\n",
    "        for epoch in range(hm_epoch):\n",
    "            epoch_loss = 0\n",
    "            for _ in range(int(mnist.train.num_examples/batch_size)):\n",
    "                epoch_x, epoch_y = mnist.train.next_batch(batch_size)\n",
    "                _, c = sess.run([opt, cost], feed_dict = {X: epoch_x, y: epoch_y})\n",
    "                epoch_loss += c\n",
    "            ## TO KEEP TRACK \n",
    "            print('Epoch {} completed out of {}... The loss {} ').format(epoch, hm_epoch, epoch_loss)\n",
    "        \n",
    "        ## TESTING THE ALGORITHM\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(y_hat,1), tf.argmax(y,1))\n",
    "        \n",
    "        ## CALCULATING THE ACCUARCY\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('ACCURACY ', accuracy.eval({X:mnist.test.images, y:mnist.test.labels}))\n",
    "\n",
    "\n",
    "## STARTING THE PROGRAM\n",
    "train_test_neural_network(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PrcFzAcK-k4E"
   },
   "source": [
    "#  NATURAL LINGUAGE PROCESSING USSING CONVENCIONAL NEURAL NETWORKS (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q05mPhzh-k4H",
    "outputId": "96ea1de3-f96e-4ef4-d228-ae08bef83c22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n",
      "Epoch  0  completed out of  10  ... The loss  33486.313720703125\n",
      "Epoch  1  completed out of  10  ... The loss  19630.473999023438\n",
      "Epoch  2  completed out of  10  ... The loss  13294.497009277344\n",
      "Epoch  3  completed out of  10  ... The loss  10420.806640625\n",
      "Epoch  4  completed out of  10  ... The loss  8969.046752929688\n",
      "Epoch  5  completed out of  10  ... The loss  5213.2529296875\n",
      "Epoch  6  completed out of  10  ... The loss  3452.4942779541016\n",
      "Epoch  7  completed out of  10  ... The loss  1985.7931823730469\n",
      "Epoch  8  completed out of  10  ... The loss  1075.1103591918945\n",
      "Epoch  9  completed out of  10  ... The loss  586.6101188659668\n",
      "ACCURACY  0.51031893\n"
     ]
    }
   ],
   "source": [
    "############################ SENTIMENT ANALISYS ############################################\n",
    "## IMPORTING LIBRARIES\n",
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "\n",
    "#******************************* PREPROCESSING THE DATA **************************************#\n",
    "\n",
    "## DEFINING THE LIMITIZER\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "## DEFINING THE N OF LINES ON THE FILE\n",
    "hm_lines = 100000\n",
    "\n",
    "## FUNCTIONS\n",
    "\n",
    "'''\n",
    "THIS FUNCTION CREATES A LEXICON OF THE WORDS PRESENTS IN THE FILES\n",
    "'''\n",
    "def def_lexicon(pos,neg):\n",
    "    \n",
    "    \n",
    "    lexicon = []\n",
    "    \n",
    "    ## GETTING EVERY WORD ON THE FILE AND POPULATING THE LEXICON\n",
    "    with open(pos, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for line in contents[: hm_lines]:\n",
    "            all_word = word_tokenize(line)\n",
    "            lexicon += list(all_word)\n",
    "    \n",
    "    with open(neg, 'r') as f:\n",
    "        contents = f.readlines()\n",
    "        for line in contents[: hm_lines]:\n",
    "            all_word = word_tokenize(line)\n",
    "            lexicon += list(all_word)\n",
    "    \n",
    "    ## STEMING THE LEXICON IN MEANINGFUL WORDS...STEAMING = CUT OF ING, ED, ETC...\n",
    "    lexicon = [lemmatizer.lemmatize(word) for word in lexicon]\n",
    "    #lexicon = [lemmatizer.lemmatize(i) for i,pos in nltk.pos_tag(lexicon)] \n",
    "    \n",
    "    ## COUNTING THE WORDS \n",
    "    words_counter = Counter(lexicon)   ##THIS RETURN A DICT WHRE THE KEY IS THE WORD AND THE VALUE HOW FREQUENT IS THAT WORD\n",
    "    \n",
    "    ## WE ARE INTERESTING AT THE WORDS THAT HAVE SOME MEDIUM FREQUENCY\n",
    "    low_limit = 50\n",
    "    high_limit = 1000\n",
    "    final_lexicon = []\n",
    "    for word in words_counter:\n",
    "        if high_limit > words_counter[word] > low_limit:\n",
    "            final_lexicon.append(word)\n",
    "    \n",
    "    ## DEBUGING PRINT\n",
    "    print (len(final_lexicon))\n",
    "    \n",
    "    ## RETURN THE LEXICON WITN THE WORDS THAT ARE RELEVANT\n",
    "    return final_lexicon\n",
    "\n",
    "'''\n",
    "THIS FUNCTION TAKE THE LEXICON CREATED BY def_lexicon AND CLASSIFIED\n",
    "'''\n",
    "def sample_handler(sample, lexicon, classification):\n",
    "    ## DEFINING THE FEATURESET\n",
    "    featureset = []\n",
    "    \n",
    "    ## POPULATING A FEATURES LIST WITH THE LIMITAZE WORDS PRESENTS IN THE SAMPLE\n",
    "    with open(sample, 'r') as fil:\n",
    "        contents = fil.readlines()\n",
    "        for line in contents[: hm_lines]:\n",
    "            current_word = word_tokenize(line.lower())\n",
    "            current_word = [lemmatizer.lemmatize(i) for i in current_word]\n",
    "            features = np.zeros(len(lexicon))\n",
    "            \n",
    "            ## GETING THE INDEX FOR EACH WORD IN THE LEXICON \n",
    "            for word in current_word:\n",
    "                if word in lexicon:\n",
    "                    word_index = lexicon.index(word.lower())\n",
    "                    features[word_index] +=1\n",
    "            \n",
    "            ##\n",
    "            features = list(features)\n",
    "            featureset.append([features, classification])\n",
    "    return featureset\n",
    "\n",
    "'''\n",
    "THIS FUNCTION CREATES THE FEATURESET AND THE LABELS BY TAKING THE OUTPUTS OF THE sample_handler\n",
    "AND THE def_lexicon FUNCTIONS...test_size IS IN %\n",
    "'''\n",
    "def create_features_labels(pos, neg, test_size = 0.1):\n",
    "    ## CREATING THE LEXICON\n",
    "    lexicon = def_lexicon(pos, neg)\n",
    "    \n",
    "    ## CREATING THE POSITIVE AND NEGATIVE FEATURESET\n",
    "    featureset = []\n",
    "    featureset += sample_handler(pos, lexicon, [1,0])\n",
    "    featureset += sample_handler(neg, lexicon, [0,1])\n",
    "    \n",
    "    ## SHUFFLING THE DATA\n",
    "    random.shuffle(featureset)\n",
    "    \n",
    "    ## DEFINING THE TRAIN DATASET AND THE TEST DATASET\n",
    "    featureset = np.array(featureset)\n",
    "    testing_size = int(test_size*len(featureset))\n",
    "    ## TRAIN DATASET\n",
    "    train_x = list(featureset[:,0][: testing_size])\n",
    "    train_y = list(featureset[:,1][: testing_size])\n",
    "    ## TEST DATASET\n",
    "    test_x = list(featureset[:,0][-testing_size :])\n",
    "    test_y = list(featureset[:,1][-testing_size :])\n",
    "    \n",
    "    ## RETURN THE DIVIDED DATASET\n",
    "    return train_x, train_y, test_x, test_y\n",
    "\n",
    "#******************************** NEURAL NETWORK MODEL ***************************************#\n",
    "\n",
    "## CREATING THE TRAIN AND TEST DATASET\n",
    "train_x, train_y, test_x, test_y = create_features_labels('pos.txt', 'neg.txt')\n",
    "\n",
    "## DEFINING THE NEURAL NETWORK HYPERPARAMETERS\n",
    "\n",
    "## NUMBER OF NEURONS AT THE HIDDEN LAYERS...FEEL FREE TO CHANGE THIS\n",
    "n_nodes_hl0 = int(len(train_x[0]))   ## THE LEN OF THE FEATURE SET\n",
    "n_nodes_hl1 = 500\n",
    "n_nodes_hl2 = 500\n",
    "n_nodes_hl3 = 500\n",
    "\n",
    "## NUMBER OF CLASSES\n",
    "n_classes = 2   ## POS OR NEG\n",
    "\n",
    "## DEFINING THE BATCH SIZE\n",
    "batch_size = 100\n",
    "\n",
    "## DEFINING THE FEATURES SET\n",
    "X = tf.placeholder('float', [None, n_nodes_hl0])\n",
    "\n",
    "## DEFINING THE LABELS\n",
    "y = tf.placeholder('float')\n",
    "\n",
    "\n",
    "'''\n",
    "THIS FUNCTION DEFINE THE MODEL FOR THE NN\n",
    "'''\n",
    "def neural_network_model(data):\n",
    "    \n",
    "    ## INICIALAZING THE WEIGTS AND BIAS FOR EACH LAYER\n",
    "    hidden_l1 = {'W': tf.Variable(tf.random_normal([n_nodes_hl0, n_nodes_hl1])), 'b': tf.Variable(tf.random_normal([n_nodes_hl1]))}\n",
    "    hidden_l2 = {'W': tf.Variable(tf.random_normal([n_nodes_hl1, n_nodes_hl2])), 'b': tf.Variable(tf.random_normal([n_nodes_hl2]))}\n",
    "    hidden_l3 = {'W': tf.Variable(tf.random_normal([n_nodes_hl2, n_nodes_hl3])), 'b': tf.Variable(tf.random_normal([n_nodes_hl3]))}\n",
    "    hidden_l4 = {'W': tf.Variable(tf.random_normal([n_nodes_hl3, n_classes])), 'b': tf.Variable(tf.random_normal([n_classes]))}\n",
    "    \n",
    "    ## CREATING THE NEURONS...z = X*W+b...a = g(z)...IN THIS CASE g() IS THE RELU\n",
    "    z1 = tf.add(tf.matmul(data, hidden_l1['W']), hidden_l1['b'])\n",
    "    a1 = tf.nn.relu(z1)\n",
    "    z2 = tf.add(tf.matmul(a1, hidden_l2['W']), hidden_l2['b'])\n",
    "    a2 = tf.nn.relu(z2)\n",
    "    z3 = tf.add(tf.matmul(a2, hidden_l3['W']), hidden_l3['b'])\n",
    "    a3 = tf.nn.relu(z3)\n",
    "    ## OUTPUT NEURON \n",
    "    output = tf.matmul(a3, hidden_l4['W']) + hidden_l4['b']\n",
    "    \n",
    "    return output\n",
    "'''\n",
    "THIS FUNCTION TRAINS AND TEST THE NN DEFINED AT THE neural_network_model FUNCTION\n",
    "'''\n",
    "def train_test_neural_network(X):\n",
    "    ## TRAINING THE NN\n",
    "    \n",
    "    ## PREDICTION\n",
    "    y_hat = neural_network_model(X)\n",
    "    \n",
    "    ## DEFINING THE COST FUNCTION...WILL BE SOFTMAX 'CAUSE THE MULTI CLASSIFICATION\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits = y_hat, labels = y))\n",
    "    \n",
    "    ## DEFINING THE OPTIMIZER \n",
    "    opt = tf.train.AdamOptimizer().minimize(cost)   ## BY DEFAULT alpha = 0.001\n",
    "    \n",
    "    ## DEFINING THE NUMBER OF ITERATIONS...FEEDFOWARD+BACKṔROP\n",
    "    hm_epoch = 10\n",
    "    \n",
    "    ## STARTING THE TENSORFLOW SESSION\n",
    "    with tf.Session() as sess:\n",
    "        ## INITIALIZING THE VARIABLES\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ## CALCULATING THE LOSS FOR EACH ITERATION\n",
    "        for epoch in range(hm_epoch):\n",
    "            epoch_loss = 0\n",
    "            i = 0\n",
    "            while i < len(train_x):\n",
    "                start = i\n",
    "                end = i+batch_size\n",
    "                batch_x = np.array(train_x[start:end])\n",
    "                batch_y = np.array(train_y[start:end])\n",
    "                _, c = sess.run([opt, cost], feed_dict = {X: batch_x, y: batch_y})\n",
    "                epoch_loss += c\n",
    "                i += batch_size\n",
    "            ## TO KEEP TRACK \n",
    "            print('Epoch ',epoch,' completed out of ',hm_epoch,' ... The loss ',epoch_loss)\n",
    "        \n",
    "        ## TESTING THE ALGORITHM\n",
    "        \n",
    "        correct = tf.equal(tf.argmax(y_hat,1), tf.argmax(y,1))\n",
    "        \n",
    "        ## CALCULATING THE ACCUARCY\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, 'float'))\n",
    "        print('ACCURACY ', accuracy.eval({X:test_x, y:test_y}))\n",
    "\n",
    "#************************************ MAIN FUNCTION ******************************************#\n",
    "if __name__ == '__main__':\n",
    "    '''\n",
    "    IF THE DATASET IS LARGE WE CANT LOAD ON THE RAM\n",
    "    SO WE SAVE A PICKLE TO ACESS THE DATASET \n",
    "    \n",
    "    train_x, train_y, test_x, test_y = create_features_labels('pos.txt', 'neg.txt')\n",
    "    \n",
    "    ## SAVING THE DATASETS IN A PICKLE SO WE CAN SAVE PROCESSING TIME IN LATERS TESTS\n",
    "    with open('sentiment_analisys_set.pickle', 'wb') as f:\n",
    "        pickle.dump([train_x, train_y, test_x, test_y], f)\n",
    "    '''\n",
    "    ## STARTING THE PROGRAM\n",
    "    train_test_neural_network(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "INpTjgUZ-k4L"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Machine_Learning_test_1-checkpoint.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
