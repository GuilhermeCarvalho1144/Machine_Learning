{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4319b006-3158-418b-bc57-0a09ddbcc450",
   "metadata": {},
   "source": [
    "# Credits\n",
    "https://github.com/ajhalthor/Transformer-Neural-Network/blob/main/Self_Attention_for_Transformer_Neural_Networks.ipynb\n",
    "https://www.youtube.com/watch?v=rPFkX5fJdRY&t=667s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce108ef0-f05c-4f07-8fdb-1328ee74d325",
   "metadata": {},
   "source": [
    "## Self-attetion numerical ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8b0ca93-a789-4c1c-b32b-251566e7377a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de80de6c-571c-4811-8270-89155a98cdc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define our inputs...for this example, our sentence will have 4 words and your key and value vector will have size 8\n",
    "L, dk, dv = 4, 8, 8\n",
    "np.random.seed(42)\n",
    "\n",
    "q = np.random.randn(L,dk)\n",
    "k = np.random.randn(L,dk)\n",
    "v = np.random.randn(L,dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d102812-37f4-4413-9e42-54b9f758d103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " [[ 0.49671415 -0.1382643   0.64768854  1.52302986 -0.23415337 -0.23413696\n",
      "   1.57921282  0.76743473]\n",
      " [-0.46947439  0.54256004 -0.46341769 -0.46572975  0.24196227 -1.91328024\n",
      "  -1.72491783 -0.56228753]\n",
      " [-1.01283112  0.31424733 -0.90802408 -1.4123037   1.46564877 -0.2257763\n",
      "   0.0675282  -1.42474819]\n",
      " [-0.54438272  0.11092259 -1.15099358  0.37569802 -0.60063869 -0.29169375\n",
      "  -0.60170661  1.85227818]]\n",
      "K\n",
      " [[-0.01349722 -1.05771093  0.82254491 -1.22084365  0.2088636  -1.95967012\n",
      "  -1.32818605  0.19686124]\n",
      " [ 0.73846658  0.17136828 -0.11564828 -0.3011037  -1.47852199 -0.71984421\n",
      "  -0.46063877  1.05712223]\n",
      " [ 0.34361829 -1.76304016  0.32408397 -0.38508228 -0.676922    0.61167629\n",
      "   1.03099952  0.93128012]\n",
      " [-0.83921752 -0.30921238  0.33126343  0.97554513 -0.47917424 -0.18565898\n",
      "  -1.10633497 -1.19620662]]\n",
      "V\n",
      " [[ 0.81252582  1.35624003 -0.07201012  1.0035329   0.36163603 -0.64511975\n",
      "   0.36139561  1.53803657]\n",
      " [-0.03582604  1.56464366 -2.6197451   0.8219025   0.08704707 -0.29900735\n",
      "   0.09176078 -1.98756891]\n",
      " [-0.21967189  0.35711257  1.47789404 -0.51827022 -0.8084936  -0.50175704\n",
      "   0.91540212  0.32875111]\n",
      " [-0.5297602   0.51326743  0.09707755  0.96864499 -0.70205309 -0.32766215\n",
      "  -0.39210815 -1.46351495]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Q\\n {q}')\n",
    "print(f'K\\n {k}')\n",
    "print(f'V\\n {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bef6f5-417b-4621-baca-1c09d81355c8",
   "metadata": {},
   "source": [
    "### Self attetion formula\n",
    "1. First, we compute the matrix multiplication between Q and K. $Q.K^T$\n",
    "2. Then we scale this matrix by dK to reduce the variance. $\\frac{Q.K^T}{\\sqrt{d_k}}$\n",
    "    1. In the decoder, we need to mask the values\n",
    "3. Then we take the softmax of these weight values to get probabilities. $Softmax(\\frac{Q.K^T}{\\sqrt{d_k}})$\n",
    "4. Finally, we multiply this probability matrix by the V matrix. $Softmax(\\frac{Q.K^T}{\\sqrt{d_k}}).V$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac398756-af63-403d-a15e-89816a7154df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.72357421,  0.40818741,  2.39601116, -1.18323729],\n",
       "       [ 5.60012069,  1.1597874 , -4.7248515 ,  2.43859568],\n",
       "       [ 1.03699903, -3.70553788, -3.0399309 ,  0.04345596],\n",
       "       [ 0.09460324,  2.97027193,  0.43247995, -0.80026704]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first step\n",
    "np.matmul(q, k.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78fb5ab6-2539-4cf3-8567-32e67e445bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before scaling 6.8375652409831655\n",
      "after scaling 0.8546956551228956\n"
     ]
    }
   ],
   "source": [
    "#second step\n",
    "#we need to divide by dk to reduce its variance\n",
    "scaled  = np.matmul(q, k.T) / np.sqrt(dk)\n",
    "\n",
    "# let's compare the variance before and after the scaling\n",
    "print(f'before scaling {np.matmul(q, k.T).var()}')\n",
    "print(f'after scaling {scaled.var()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "770ea5c2-d711-4f64-a02c-e0a6271a3bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attetion vector\n",
      "[[0.08431243 0.25513027 0.51521078 0.14534652]\n",
      " [0.64059204 0.1332861  0.01664257 0.2094793 ]\n",
      " [0.47006414 0.08789379 0.11121405 0.33082801]\n",
      " [0.17794451 0.49185018 0.20052305 0.12968226]]\n"
     ]
    }
   ],
   "source": [
    "#third step\n",
    "#we compute the softmax of the scaled matrix so all the columns sum to one\n",
    "def softmax(x):\n",
    "    return (np.exp(x).T/np.sum(np.exp(x), axis=1)).T\n",
    "\n",
    "attetion = softmax(scaled)\n",
    "print(f'Attetion vector\\n{attetion}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "033ebcda-e123-4979-9579-8f3973c7c54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New values\n",
      " [[-0.1308104   0.77212573  0.10108921  0.16807328 -0.46588684 -0.43681263\n",
      "   0.46851458 -0.42075407]\n",
      " [ 0.40109276  1.19080398 -0.35037302  0.94668908  0.08274232 -0.53010106\n",
      "   0.17683369  0.41923385]\n",
      " [ 0.17910025  0.98456145 -0.06763014  0.80678092 -0.14453166 -0.49373081\n",
      "   0.15002954  0.10067088]\n",
      " [ 0.01421368  1.14907671 -0.99239485  0.60451701 -0.14600018 -0.40496816\n",
      "   0.24215067 -0.82777073]]\n"
     ]
    }
   ],
   "source": [
    "#Final step\n",
    "#we multiply the attention vector for the values so we get the new values\n",
    "new_v = np.matmul(attetion, v)\n",
    "print(f'New values\\n {new_v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c89b7ee-a354-4953-90f7-d659e0d912c8",
   "metadata": {},
   "source": [
    "### For the decoder we need to apply a mask functions before compute the probabilite vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9d70b693-ceb4-4d84-99d7-2a9a9941a882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked new values\n",
      " [[ 0.81252582  1.35624003 -0.07201012  1.0035329   0.36163603 -0.64511975\n",
      "   0.36139561  1.53803657]\n",
      " [ 0.66641301  1.39213367 -0.51081002  0.97225045  0.31434319 -0.58550834\n",
      "   0.31495603  0.93081668]\n",
      " [ 0.52954961  1.21756173 -0.14905901  0.7267579   0.1310981  -0.57583252\n",
      "   0.41805381  0.87397953]\n",
      " [ 0.01421368  1.14907671 -0.99239485  0.60451701 -0.14600018 -0.40496816\n",
      "   0.24215067 -0.82777073]]\n"
     ]
    }
   ],
   "source": [
    "#for the decoder the steps are the following\n",
    "#compute mask\n",
    "mask = np.tril(np.ones((L,L)))\n",
    "mask[mask == 0] = -np.inf\n",
    "mask[mask == 1] = 0\n",
    "masked_scaled = scaled + mask\n",
    "masked_attetion = softmax(masked_scaled)\n",
    "masked_new_v = np.matmul(masked_attetion, v)\n",
    "print(f'Masked new values\\n {masked_new_v}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74405f77-fa41-454c-864e-dc4a21487524",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
