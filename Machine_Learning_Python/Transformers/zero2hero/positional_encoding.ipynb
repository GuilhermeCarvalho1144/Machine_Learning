{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "240c9e80-cbd3-49d3-8fd6-ee5b1d139f10",
   "metadata": {},
   "source": [
    "# Credits\n",
    "https://www.youtube.com/watch?v=rPFkX5fJdRY&t=1179s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b6ad6-8c5a-411f-8d6f-63d7a6102441",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84109681-d789-4951-b006-1680f4716508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f64ae98-5572-4f68-b6c3-9c0d95a1cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first we need to define the max input senquence(usually thousands) and the d_model(the size of the embadding layer)\n",
    "max_sequence_length = 10\n",
    "d_model = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc805b3-f703-4984-a7b4-d90f302d556f",
   "metadata": {},
   "source": [
    "### Formula of the postinal encoding\n",
    "Same as the papaer\n",
    "\n",
    "$$\n",
    "PE_{(pos , 2i)} = \\sin({\\frac{pos}{10000^{2i/d_{model}}}}) \n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "PE_{(pos , 2i+1)} = \\cos({\\frac{pos}{10000^{2i/d_{model}}}})\n",
    "$$\n",
    "\n",
    "\n",
    "we can rewrite it as follows\n",
    "\n",
    "\n",
    "$$\n",
    "PE_{(pos , i)} = sin({\\frac{pos}{10000^{i/d_{model}}}}) \n",
    "$$\n",
    "\n",
    "$$PE_{(pos , 1)} = \\cos({\\frac{pos}{10000^{i-1/d_{model}}}})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "caaed93d-7792-41b6-a7ce-a1611fdbfc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even_denominator: tensor([  1.0000,  21.5443, 464.1590])\n",
      "odd_denominator: tensor([  1.0000,  21.5443, 464.1590])\n"
     ]
    }
   ],
   "source": [
    "# Let evaluate only de demominator for the even and odd number\n",
    "even_array = torch.arange(0, d_model, 2).float()\n",
    "odd_array = torch.arange(1, d_model, 2).float()\n",
    "\n",
    "even_denominator = torch.pow(10E3, even_array/d_model)\n",
    "print(f'even_denominator: {even_denominator}')\n",
    "\n",
    "odd_denominator = torch.pow(10E3, (odd_array-1)/d_model)\n",
    "print(f'odd_denominator: {odd_denominator}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fbf7c80-ec4b-4918-a742-3592d81d632c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positional:\n",
      " tensor([[0.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]])\n"
     ]
    }
   ],
   "source": [
    "#since the values are the same we can use only one denominator\n",
    "denominator  = odd_denominator\n",
    "#now let's create our positional vector\n",
    "positional = torch.arange(max_sequence_length, dtype=torch.float).reshape(max_sequence_length,1)\n",
    "print(f'Positional:\\n {positional}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6d974ec1-c624-4528-9c63-5a30f7753ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "even_PE:\n",
      " tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.8415,  0.0464,  0.0022],\n",
      "        [ 0.9093,  0.0927,  0.0043],\n",
      "        [ 0.1411,  0.1388,  0.0065],\n",
      "        [-0.7568,  0.1846,  0.0086],\n",
      "        [-0.9589,  0.2300,  0.0108],\n",
      "        [-0.2794,  0.2749,  0.0129],\n",
      "        [ 0.6570,  0.3192,  0.0151],\n",
      "        [ 0.9894,  0.3629,  0.0172],\n",
      "        [ 0.4121,  0.4057,  0.0194]])\n",
      "odd_PE:\n",
      " tensor([[ 1.0000,  1.0000,  1.0000],\n",
      "        [ 0.5403,  0.9989,  1.0000],\n",
      "        [-0.4161,  0.9957,  1.0000],\n",
      "        [-0.9900,  0.9903,  1.0000],\n",
      "        [-0.6536,  0.9828,  1.0000],\n",
      "        [ 0.2837,  0.9732,  0.9999],\n",
      "        [ 0.9602,  0.9615,  0.9999],\n",
      "        [ 0.7539,  0.9477,  0.9999],\n",
      "        [-0.1455,  0.9318,  0.9999],\n",
      "        [-0.9111,  0.9140,  0.9998]])\n"
     ]
    }
   ],
   "source": [
    "#now lest compute the even and odd positional embeddings\n",
    "even_PE = torch.sin(positional/denominator)\n",
    "odd_PE = torch.cos(positional/denominator)\n",
    "\n",
    "print(f'even_PE:\\n {even_PE}')\n",
    "\n",
    "print(f'odd_PE:\\n {odd_PE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c3c0ec-6e57-4976-920f-d6c6a05079aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we need to reorder them\n",
    "stacked = torch.stack([even_PE, odd_PE], dim=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
